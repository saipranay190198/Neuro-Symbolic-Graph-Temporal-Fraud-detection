Neuro-Symbolic Graph Temporal (NSGT) Fraud Detection – Methods & Minimal Code Scaffold
A concise, research-paper–ready Methods section plus a runnable-ish Python scaffold you can extend into experiments (PaySim or similar transactional data).

1) Methods (paper-ready)
1.1 Problem Setting
We consider streaming payment events
e=(u,v,a,t,c)
e=(u,v,a,t,c)
representing a transfer from source account
u
u
to destination account
v
v
with amount
a
a
, timestamp
t
t
, and context
c
c
(channel, merchant type, device hash, etc.). The task is to flag transactions as fraudulent in real time with latency < 100 ms while providing case-level human-readable explanations.
1.2 Model Overview (NSGT)
Our approach ensembles three complementary components:
1. Self-Supervised Temporal Encoder (SSTE): Learns account-level temporal embeddings from unlabeled event streams using next-event and time-gap prediction, improving label efficiency and robustness to label noise.
2. Graph Risk Propagation (GRP): Builds a rolling transaction graph and diffuses risk across high-risk motifs (fan-in/out bursts, short paths to cash-out nodes) using time-decayed label propagation and/or GraphSAGE.
3. Symbolic Rule Inducer (SRI): Distills the ensemble’s behavior into sparse decision rules that serve as per-alert rationales and audit artifacts.
The final fraud probability is
y
^
=
σ
(
w
1
f
SSTE
+
w
2
f
GRP
+
w
3
f
SRI
)
\hat{y}=\sigma(w_1 f_{\text{SSTE}} + w_2 f_{\text{GRP}} + w_3 f_{\text{SRI}})
, with weights
w
i
w_i
learned on a validation fold.
1.3 Self-Supervised Temporal Encoder (SSTE)
We model each account’s event sequence with a lightweight GRU/TCN. Pretraining objectives:
* Next-amount & next-type regression/classification (teacher forcing).
* Time-gap prediction (log-time regression).
* Contrastive objective (InfoNCE): discriminate true vs. permuted subsequences within a time window. The SSTE outputs a fixed-size embedding per (account, time) and per transaction.
1.4 Graph Risk Propagation (GRP)
We construct a directed, time-sliced graph
G
t
=
(
V
,
E
t
)
G_t=(V,E_t)
over a rolling window (e.g., last 72 hours). Nodes are accounts/devices; edges are transfers and shared-attribute links (e.g., same device/IP). We compute:
* Velocity features: fan-in/out counts with half-life decay.
* Motif scores: burstiness, 2/3-hop reachability to known cash-out nodes.
* Risk diffusion: time-decayed label propagation or GraphSAGE trained on historical labels to obtain node/edge embeddings and neighbor-risk scores.
1.5 Symbolic Rule Inducer (SRI)
We fit a compact rule model (Falling Rule Lists / rulefit-style) on the ensemble scores and key features (SHAP-ranked) to produce human-readable predicates. We enforce monotonic constraints on selected features (e.g., higher fan-out should not reduce risk). For each alert, we output the top firing rule(s) with feature attributions.
1.6 Training & Calibration
* Stage A (Unsupervised): Pretrain SSTE on all events.
* Stage B (Features): Generate graph features/embeddings on rolling windows.
* Stage C (Supervised): Train a gradient-boosted tree (XGBoost/LightGBM) on concatenated features (SSTE, graph, tabular). Optimize PR-AUC and Recall@FPR≤1% with class weights or focal loss.
* Stage D (Distillation): Distill into SRI; ensure fidelity > 90% vs. ensemble on a holdout set. Calibrate probabilities with isotonic regression.
1.7 Evaluation
Metrics: PR-AUC, Recall@FPR≤1%, Brier score (calibration), cost-utility (recovered − review − friction). Robustness: weekly backtests, feature drift (PSI), stability of rules (Jaccard vs. time). Latency is profiled end-to-end with synthetic throughput tests.
1.8 Deployment Sketch
Kafka → Feature Store (Feast/Redis) → SSTE microservice (GRU) → Graph cache (rolling) → XGB scorer → SRI explainer. Actions: step-up auth / queue / pass. Observability: log rule IDs, SHAP, and outcome for feedback learning.
1.9 Limitations
PaySim domain gap; approximate device/IP links; privacy constraints for cross-bank graphs. Future work: vertical federated learning across institutions; causal counterfactual checks for rule stability.

2) Minimal Code Scaffold (Python)
Structure you can copy into a repo. The code is modular with placeholders; fill in I/O and training data specifics.
project/
  configs/
    default.yaml
  nsgt/
    __init__.py
    data.py
    features.py
    sste.py
    graph.py
    trainer.py
    distill.py
    eval.py
    utils.py
  scripts/
    pretrain_sste.py
    build_graph_feats.py
    train_supervised.py
    distill_rules.py
    evaluate.py
  notebooks/
    00_quickstart.ipynb
2.1 data.py
from dataclasses import dataclass
from typing import Dict, Iterator
import pandas as pd

@dataclass
class Event:
    src: str
    dst: str
    amount: float
    ts: int  # epoch seconds
    channel: str
    device: str | None = None
    label: int | None = None  # 1=fraud, 0=legit, None=unlabeled

class EventStream:
    def __init__(self, df: pd.DataFrame):
        self.df = df.sort_values('ts')

    def iter_batches(self, batch_seconds: int) -> Iterator[pd.DataFrame]:
        start = self.df.ts.min()
        end = self.df.ts.max()
        t = start
        while t <= end:
            mask = (self.df.ts >= t) & (self.df.ts < t + batch_seconds)
            yield self.df.loc[mask]
            t += batch_seconds
2.2 features.py
import numpy as np
import pandas as pd

def tabular_features(df: pd.DataFrame) -> pd.DataFrame:
    # Simple rolling stats per src
    out = df.copy()
    out['amt_log'] = np.log1p(out['amount'])
    out['hour'] = pd.to_datetime(out['ts'], unit='s').dt.hour
    # z-score per src account
    out['src_amt_mean'] = out.groupby('src')['amount'].transform(lambda x: x.rolling(50, min_periods=1).mean())
    out['src_amt_std'] = out.groupby('src')['amount'].transform(lambda x: x.rolling(50, min_periods=1).std().fillna(0.0))
    out['amt_z'] = (out['amount'] - out['src_amt_mean']) / (out['src_amt_std'] + 1e-6)
    return out[['amt_log','hour','amt_z']]
2.3 sste.py
import torch
import torch.nn as nn

class TinyGRU(nn.Module):
    def __init__(self, in_dim: int, hidden: int = 64, emb_dim: int = 64):
        super().__init__()
        self.gru = nn.GRU(in_dim, hidden, batch_first=True)
        self.proj = nn.Linear(hidden, emb_dim)
    def forward(self, x):  # x: (B,T,F)
        h,_ = self.gru(x)
        z = self.proj(h[:, -1, :])  # last step embedding
        return z

class SSTEHead(nn.Module):
    def __init__(self, emb_dim: int):
        super().__init__()
        self.next_amt = nn.Linear(emb_dim, 1)
        self.next_gap = nn.Linear(emb_dim, 1)
        self.cls = nn.Linear(emb_dim, 16)  # e.g., next-type classes
    def forward(self, z):
        return {
            'next_amt': self.next_amt(z),
            'next_gap': self.next_gap(z),
            'cls': self.cls(z)
        }
2.4 graph.py
import pandas as pd
import numpy as np
from collections import defaultdict

# Simple time-decayed fan-in/out and neighbor risk features

def graph_features(df: pd.DataFrame, half_life_minutes: int = 60) -> pd.DataFrame:
    hl = half_life_minutes * 60
    alpha = np.log(2) / hl
    df = df.sort_values('ts')
    fin = defaultdict(float)
    fout = defaultdict(float)
    feats = []
    for row in df.itertuples(index=False):
        t = row.ts
        src, dst = row.src, row.dst
        # decay
        for k in list(fin.keys()):
            fin[k] *= np.exp(-alpha)
        for k in list(fout.keys()):
            fout[k] *= np.exp(-alpha)
        # update
        fout[src] += 1.0
        fin[dst] += 1.0
        feats.append({'fan_out': fout[src], 'fan_in': fin[dst]})
    return pd.DataFrame(feats)
2.5 trainer.py
import numpy as np
import pandas as pd
from xgboost import XGBClassifier
from sklearn.metrics import average_precision_score
from sklearn.calibration import CalibratedClassifierCV

class SupervisedEnsembler:
    def __init__(self):
        self.model = XGBClassifier(
            n_estimators=400, max_depth=5, learning_rate=0.05,
            subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,
            scale_pos_weight=10.0
        )
        self.calib = None

    def fit(self, X_train, y_train, X_val, y_val):
        self.model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)
        self.calib = CalibratedClassifierCV(self.model, method='isotonic', cv='prefit')
        self.calib.fit(X_val, y_val)
        ap = average_precision_score(y_val, self.calib.predict_proba(X_val)[:,1])
        print(f"Val PR-AUC: {ap:.4f}")

    def predict_proba(self, X):
        return self.calib.predict_proba(X)[:,1]
2.6 distill.py
import numpy as np
from sklearn.tree import DecisionTreeClassifier

class RuleDistiller:
    def __init__(self, max_depth=4, min_samples_leaf=100):
        self.tree = DecisionTreeClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf)
    def fit(self, X, y_scores):
        # binarize with threshold selected for target FPR
        thr = np.percentile(y_scores, 90)  # placeholder
        y_bin = (y_scores >= thr).astype(int)
        self.tree.fit(X, y_bin)
    def explain(self, x_row):
        # simple path explanation
        node_indicator = self.tree.decision_path([x_row])
        return str(node_indicator.indices)
2.7 eval.py
from sklearn.metrics import precision_recall_curve, auc

def pr_auc(y_true, y_score):
    p, r, _ = precision_recall_curve(y_true, y_score)
    return auc(r, p)
2.8 Example training script (scripts/train_supervised.py)
import pandas as pd
from nsgt.features import tabular_features
from nsgt.graph import graph_features
from nsgt.trainer import SupervisedEnsembler

# 1) Load
df = pd.read_parquet('data/transactions.parquet')  # columns: src,dst,amount,ts,label

# 2) Features
X_tab = tabular_features(df)
X_graph = graph_features(df)
X = pd.concat([X_tab, X_graph], axis=1).fillna(0.0)
y = df['label'].values

# 3) Split
split = int(0.8 * len(df))
X_train, y_train = X.iloc[:split], y[:split]
X_val, y_val = X.iloc[split:], y[split:]

# 4) Train
clf = SupervisedEnsembler()
clf.fit(X_train, y_train, X_val, y_val)

# 5) Save probabilities
proba = clf.predict_proba(X_val)
pd.DataFrame({'score': proba}).to_csv('out/val_scores.csv', index=False)

3) How to run (prototype)
1. Export PaySim (or any dataset) to data/transactions.parquet with columns: src, dst, amount, ts, label (ints/floats; ts in epoch seconds).
2. python scripts/train_supervised.py to train the XGBoost head on simple features.
3. Extend by:
    * Pretraining sste.py (build a loader of per-account sequences; save embeddings).
    * Replacing graph_features with PyG/DGL-based embeddings.
    * Running distill.py on X and the ensemble scores to get rules.
4) Expected deliverables
* Reproducible configs (YAML) for ablations (w/ & w/o SSTE, GRP, SRI).
* Metrics notebook with PR/ROC, Recall@FPR≤1%, calibration curves.
* Casebook of top-50 alerts with rule explanations and feature attributions.
